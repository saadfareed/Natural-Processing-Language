{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name:  Muhammad Saad Fareed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Reg#:2018-CS-632"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".میرا\n",
      "نام\n",
      "محمد\n",
      "سعد\n",
      "فرید\n",
      "ہے\n",
      "اور\n",
      "میں\n",
      "کمپیوٹر\n",
      "سائنس\n",
      "شعبہ\n",
      "کا\n",
      "طالب\n",
      "علم\n",
      "ہو.\n",
      "ںمیں\n",
      "قدرتی\n",
      "پروسیسنگ\n",
      "کی\n",
      "زبان\n",
      "سیکھ\n",
      "رہا\n",
      "ہوں\n"
     ]
    }
   ],
   "source": [
    "def tokenize(file_path):\n",
    "    with open(file_path,'r') as file:\n",
    "        for line in file:        \n",
    "            for word in line.split():\n",
    "                print(word)  \n",
    "tokenize('file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "def tokennumber(file_path):\n",
    "    corpusFileName=file_path \n",
    "    corpusFile=open(corpusFileName);\n",
    "    data=corpusFile.read()\n",
    "    corpusTokens=data.split()\n",
    "    nooftokens=int(len(corpusTokens))\n",
    "    return nooftokens\n",
    "print(tokennumber('file.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct words: 14\n",
      "Words: 19\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "f = open(\"result.txt\")\n",
    "my_text=f.read()\n",
    "match = re.findall(r'[a-zA-Z\\’\\d\\'\\-]+',my_text)\n",
    "data={}\n",
    "for word in match:\n",
    "    if word not in data:\n",
    "        data[word]=1\n",
    "    else:\n",
    "        data[word]+=1\n",
    "print(\"Distinct words:\",len(data.keys()))\n",
    "print(\"Words:\",  len(match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Bigram without Taking input From User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['saad'], ['fareed']], [['fareed'], ['is']], [['is'], ['my']], [['my'], ['name']], [['happy'], ['birthday']], [['birthday'], ['dear']], [['dear'], ['saad']], [['saad'], ['fareed']]]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "def bigram(sentence):\n",
    "    for line in sentence:\n",
    "        arr = line.split()\n",
    "        for i in range(len(arr)-1):\n",
    "            result.append([[arr[i]], [arr[i+1]]])\n",
    "            \n",
    "sentence= ['saad fareed is my name','happy birthday dear saad fareed']\n",
    "bigram(sentence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Bigram by Taking input from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['.میرا'], ['نام']], [['نام'], ['محمد']], [['محمد'], ['سعد']], [['سعد'], ['فرید']], [['فرید'], ['ہے']], [['ہے'], ['اور']], [['اور'], ['میں']], [['میں'], ['کمپیوٹر']], [['کمپیوٹر'], ['سائنس']], [['سائنس'], ['شعبہ']], [['شعبہ'], ['کا']], [['کا'], ['طالب']], [['طالب'], ['علم']], [['علم'], ['ہو.']], [['ںمیں'], ['قدرتی']], [['قدرتی'], ['پروسیسنگ']], [['پروسیسنگ'], ['کی']], [['کی'], ['زبان']], [['زبان'], ['سیکھ']], [['سیکھ'], ['رہا']], [['رہا'], ['ہوں']]]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "def bigram(sentence):\n",
    "    for line in sentence:\n",
    "        arr = line.split()\n",
    "        for i in range(len(arr)-1):\n",
    "            result.append([[arr[i]], [arr[i+1]]])\n",
    "f = open(\"file.txt\", \"r\")\n",
    "bigram(f)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Ngrams without Taking input From User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['saad', 'fareed', 'is'], ['fareed', 'is', 'my'], ['is', 'my', 'name']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngrams(sentence, n):\n",
    "    sentence = sentence.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(sentence)-n+1):\n",
    "        output.append(sentence[i:i+n])\n",
    "    return output\n",
    "ngrams('saad fareed is my name', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Ngrams by taking Order from User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Order 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['saad', 'fareed'], ['fareed', 'is'], ['is', 'my'], ['my', 'name']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngrams(sentence, n):\n",
    "    sentence = sentence.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(sentence)-n+1):\n",
    "        output.append(sentence[i:i+n])\n",
    "    return output\n",
    "order=int(input(\"Enter the Order \"))\n",
    "ngrams('saad fareed is my name', order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Ngrams From Files and Taking Order from User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Order 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['.میرا', 'نام', 'محمد'],\n",
       " ['نام', 'محمد', 'سعد'],\n",
       " ['محمد', 'سعد', 'فرید'],\n",
       " ['سعد', 'فرید', 'ہے'],\n",
       " ['فرید', 'ہے', 'اور'],\n",
       " ['ہے', 'اور', 'میں'],\n",
       " ['اور', 'میں', 'کمپیوٹر'],\n",
       " ['میں', 'کمپیوٹر', 'سائنس'],\n",
       " ['کمپیوٹر', 'سائنس', 'شعبہ'],\n",
       " ['سائنس', 'شعبہ', 'کا'],\n",
       " ['شعبہ', 'کا', 'طالب'],\n",
       " ['کا', 'طالب', 'علم'],\n",
       " ['طالب', 'علم', 'ہو.\\nںمیں'],\n",
       " ['علم', 'ہو.\\nںمیں', 'قدرتی'],\n",
       " ['ہو.\\nںمیں', 'قدرتی', 'پروسیسنگ'],\n",
       " ['قدرتی', 'پروسیسنگ', 'کی'],\n",
       " ['پروسیسنگ', 'کی', 'زبان'],\n",
       " ['کی', 'زبان', 'سیکھ'],\n",
       " ['زبان', 'سیکھ', 'رہا'],\n",
       " ['سیکھ', 'رہا', 'ہوں']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngrams(sentence, n):\n",
    "    sentence = sentence.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(sentence)-n+1):\n",
    "        output.append(sentence[i:i+n])\n",
    "    return output\n",
    "order=int(input(\"Enter the Order \"))\n",
    "f = open(\"file.txt\")\n",
    "ngrams(f.read(), order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Num of Tokens as well as Ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Enter the Order 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['.میرا', 'نام'],\n",
       " ['نام', 'محمد'],\n",
       " ['محمد', 'سعد'],\n",
       " ['سعد', 'فرید'],\n",
       " ['فرید', 'ہے'],\n",
       " ['ہے', 'اور'],\n",
       " ['اور', 'میں'],\n",
       " ['میں', 'کمپیوٹر'],\n",
       " ['کمپیوٹر', 'سائنس'],\n",
       " ['سائنس', 'شعبہ'],\n",
       " ['شعبہ', 'کا'],\n",
       " ['کا', 'طالب'],\n",
       " ['طالب', 'علم'],\n",
       " ['علم', 'ہو.\\nںمیں'],\n",
       " ['ہو.\\nںمیں', 'قدرتی'],\n",
       " ['قدرتی', 'پروسیسنگ'],\n",
       " ['پروسیسنگ', 'کی'],\n",
       " ['کی', 'زبان'],\n",
       " ['زبان', 'سیکھ'],\n",
       " ['سیکھ', 'رہا'],\n",
       " ['رہا', 'ہوں']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ngrams(sentence, n):\n",
    "    sentence = sentence.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(sentence)-n+1):\n",
    "        output.append(sentence[i:i+n])\n",
    "    return output\n",
    "def tokennumber(file_path):\n",
    "    corpusFileName=file_path \n",
    "    corpusFile=open(corpusFileName);\n",
    "    data=corpusFile.read()\n",
    "    corpusTokens=data.split()\n",
    "    nooftokens=int(len(corpusTokens))\n",
    "    return nooftokens\n",
    "print(tokennumber('file.txt'))\n",
    "order=int(input(\"Enter the Order \"))\n",
    "f = open(\"file.txt\")\n",
    "ngrams(f.read(), order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Bigram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It will generate Bigrams of the language \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['.میرا', 'نام'],\n",
       " ['نام', 'محمد'],\n",
       " ['محمد', 'سعد'],\n",
       " ['سعد', 'فرید'],\n",
       " ['فرید', 'ہے'],\n",
       " ['ہے', 'اور'],\n",
       " ['اور', 'میں'],\n",
       " ['میں', 'کمپیوٹر'],\n",
       " ['کمپیوٹر', 'سائنس'],\n",
       " ['سائنس', 'شعبہ'],\n",
       " ['شعبہ', 'کا'],\n",
       " ['کا', 'طالب'],\n",
       " ['طالب', 'علم'],\n",
       " ['علم', 'ہو.\\nںمیں'],\n",
       " ['ہو.\\nںمیں', 'قدرتی'],\n",
       " ['قدرتی', 'پروسیسنگ'],\n",
       " ['پروسیسنگ', 'کی'],\n",
       " ['کی', 'زبان'],\n",
       " ['زبان', 'سیکھ'],\n",
       " ['سیکھ', 'رہا'],\n",
       " ['رہا', 'ہوں']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigrams(sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(sentence)-1):\n",
    "        output.append(sentence[i:i+2])\n",
    "    return output\n",
    "print(\"It will generate Bigrams of the language \")\n",
    "f = open(\"file.txt\")\n",
    "bigrams(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Probability by using Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('saad', 'fareed') : 2\n",
      "('fareed', 'is') : 2\n",
      "('is', 'my') : 2\n",
      "('my', 'name') : 1\n",
      "('name', 'i') : 1\n",
      "('i', 'am') : 1\n",
      "('am', 'a') : 1\n",
      "('a', 'student') : 1\n",
      "('student', 'of') : 1\n",
      "('of', 'computer') : 1\n",
      "('computer', 'science') : 1\n",
      "('science', 'saad') : 1\n",
      "('my', 'brand') : 1\n",
      "('brand', 'name') : 1\n",
      "('name', 'aslo') : 1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def compute_probabilty(file_name):\n",
    "    # count bigrams\n",
    "    bigrams = {}\n",
    "    words_punct = file_name.split()\n",
    "    words = [ w.strip(string.punctuation).lower() for w in words_punct ]\n",
    "    for index, word in enumerate(words):\n",
    "        if index < len(words) - 1:\n",
    "            w1 = words[index]\n",
    "            w2 = words[index + 1]\n",
    "            bigram = (w1, w2)\n",
    "            if bigram in bigrams:\n",
    "                bigrams[ bigram ] = bigrams[ bigram ] + 1\n",
    "            else:\n",
    "                bigrams[ bigram ] = 1\n",
    "            \n",
    "    sorted_bigrams = sorted(bigrams.items(), key = lambda pair:pair[1], reverse = True)\n",
    "\n",
    "    for bigram, count in sorted_bigrams:\n",
    "        print(bigram, \":\", count)\n",
    "\n",
    "f=open(\"result.txt\")\n",
    "compute_probabilty(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Probabilty without taking input from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('my', 'name') : 2\n",
      "('name', 'is') : 1\n",
      "('is', 'saad') : 1\n",
      "('saad', 'and') : 1\n",
      "('and', 'i') : 1\n",
      "('i', 'am') : 1\n",
      "('am', 'a') : 1\n",
      "('a', 'student') : 1\n",
      "('student', 'of') : 1\n",
      "('of', 'cs') : 1\n",
      "('cs', 'saad') : 1\n",
      "('saad', 'is') : 1\n",
      "('is', 'my') : 1\n",
      "('my', 'brand') : 1\n",
      "('brand', 'name') : 1\n",
      "('name', 'also') : 1\n",
      "('also', 'my') : 1\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def computer_probabilty(file_name):\n",
    "    # count bigrams\n",
    "    bigrams = {}\n",
    "    words_punct = file_name.split()\n",
    "    words = [ w.strip(string.punctuation).lower() for w in words_punct ]\n",
    "    for index, word in enumerate(words):\n",
    "        if index < len(words) - 1:\n",
    "            w1 = words[index]\n",
    "            w2 = words[index + 1]\n",
    "            bigram = (w1, w2)\n",
    "            if bigram in bigrams:\n",
    "                bigrams[ bigram ] = bigrams[ bigram ] + 1\n",
    "            else:\n",
    "                bigrams[ bigram ] = 1\n",
    "            \n",
    "    sorted_bigrams = sorted(bigrams.items(), key = lambda pair:pair[1], reverse = True)\n",
    "\n",
    "    for bigram, count in sorted_bigrams:\n",
    "        print(bigram, \":\", count)\n",
    "\n",
    "sentence=\"My name is saad and i am a student of CS. Saad is my brand name also. My name \"\n",
    "computer_probabilty(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
